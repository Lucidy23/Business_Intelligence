{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden der Parquet Datei in einen Dataframe\n",
    "df = pd.read_parquet('escooter_history.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Übersicht des DF\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausweisung der Datentypen der Spalten\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prüfung nach Null-Werten\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prüfen nach Duplikaten\n",
    "df_dup = df[df.duplicated()]\n",
    "df_dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"datetime\"] = df[\"datetime\"] - datetime.timedelta(2922)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_hourly = df_dup[['holiday']].groupby(df['datetime'].dt.hour).count().rename(columns={'holiday':'count'})\n",
    "dup_hourly['%'] = dup_hourly['count'] / dup_hourly['count'].sum() * 100\n",
    "dup_hourly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Das Ergebniss scheint plausibel zu sein, da die meisten Duplikate während der Rush-Hour vorkommen, deswegen behalten wir die Duplikate im DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(dup_hourly.reset_index(), x = 'datetime', y = '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weather_name'] = df['weather']\n",
    "df['weather'] = df['weather'].replace({'heacy rain or thunderstorm or snow or ice pallets' : 'heavy rain or thunderstorm or snow or ice pallets'})\n",
    "df['weather'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# casting registered_customer as int\n",
    "df['registered_customer'] = df['registered_customer'].astype(int)\n",
    "# CATEGORICAL DATA?\n",
    "df.groupby(df['weather']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellung eines dummy DF für die Wetter-Attribute\n",
    "df_dummy = pd.get_dummies(df[['datetime', 'weather']])\n",
    "df_dummy['Datum'] = df_dummy.datetime.dt.date\n",
    "df_dummy = df_dummy.groupby(['Datum']).sum()\n",
    "df_dummy = df_dummy.reset_index()\n",
    "df_dummy['Datum'] = pd.to_datetime(df_dummy.Datum)\n",
    "\n",
    "def get_max_dummy(val):\n",
    "    st = val.idxmax()\n",
    "    val[st] = 1\n",
    "    li = ['weather_clear, few clouds', 'weather_cloudy, mist' ,'weather_heavy rain or thunderstorm or snow or ice pallets','weather_light snow or rain or thunderstorm']\n",
    "    li.remove(st)\n",
    "    for el in li:\n",
    "        val[el] = 0\n",
    "    return val\n",
    "\n",
    "df_dummyt = df_dummy.drop('Datum', axis = 1).apply(get_max_dummy, axis=1)\n",
    "df_dummy = pd.merge(df_dummy[['Datum']].reset_index(), df_dummyt.reset_index(), on='index')\n",
    "df_dummy = df_dummy.drop('index', axis = 1)\n",
    "df_dummy = df_dummy.set_index('Datum')\n",
    "df_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode weather using map\n",
    "### SCHREIBFEHLER verbessern\n",
    "df['weather_name'] = df['weather'].map({'clear, few clouds':0,'cloudy, mist':1,'light snow or rain or thunderstorm':2,'heavy rain or thunderstorm or snow or ice pallets':3})\n",
    "df['weather_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_hourly = df[['datetime','registered_customer']].resample('1H',on='datetime').agg({'datetime':'count','registered_customer':'sum'}).rename(columns={'datetime':'Bookings'})\n",
    "df_count_hourly.head()\n",
    "df_count_hourly['unregistered_customer'] = df_count_hourly['Bookings'] - df_count_hourly['registered_customer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_hourly = df.resample('1H',on='datetime').mean()\n",
    "df_mean_hourly.drop(['registered_customer'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hourly = pd.merge(df_count_hourly,df_mean_hourly,on='datetime').reset_index()\n",
    "df_hourly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### löschen von unnötigen spalten\n",
    "df_hourly['day_of_week'] = df_hourly['datetime'].dt.dayofweek\n",
    "df_hourly['weekday'] = df_hourly['datetime'].dt.day_name()\n",
    "season_dict = { 1 : \"Winter\", 2 : \"Spring\", 3 : \"Summer\", 4 :\"Autumn\"}\n",
    "df_hourly[\"season\"] = df_hourly[\"datetime\"].dt.month % 12 // 3 + 1\n",
    "df_hourly[\"season\"] =  df_hourly[\"season\"].map(season_dict)\n",
    "df_hourly['week_of_year'] =  df_hourly['datetime'].dt.isocalendar().week\n",
    "df_hourly['hour'] = df_hourly['datetime'].dt.hour\n",
    "df_hourly['day'] = df_hourly['datetime'].dt.day\n",
    "df_hourly['month'] = df_hourly['datetime'].dt.month\n",
    "df_hourly['year'] =  df_hourly['datetime'].dt.year\n",
    "df_hourly['date'] = df_hourly['datetime'].dt.date\n",
    "\n",
    "#df_hourly['weekday'] = df['weekday'].map({''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hourly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hourly['day_of_week'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hourly.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns where no bookings happend\n",
    "df_hourly = df_hourly[~(df_hourly['Bookings']==0)]\n",
    "df_hourly\n",
    "#df_hourly.fillna(method='ffill',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop datetime not needed right now()\n",
    "df_hourly = df_hourly.drop(['datetime'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hourly.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame erstellen basierend auf täglichen Werten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day = df.set_index('datetime')\n",
    "df_day.index = pd.to_datetime(df_day.index)\n",
    "df_day_mean = df_day.resample('d').mean()\n",
    "df_day_mean = df_day_mean.drop('registered_customer', axis = 1)\n",
    "df_day_bookings = df_day.resample('d')[['temp', 'registered_customer']].agg({'temp' : 'count', 'registered_customer' : 'sum'})\n",
    "df_day_bookings.columns = ['Bookings', 'registered_customer']\n",
    "df_day = pd.merge(df_day_mean.reset_index(), df_day_bookings.reset_index(), on = 'datetime')\n",
    "df_day['weather'] = df_day['weather'].astype('int')\n",
    "df_day['unregistered_customer'] = df_day.Bookings - df_day.registered_customer\n",
    "df_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hinzufügen der Dummy Wetter Attribute\n",
    "df_day = pd.merge(df_day, df_dummy.reset_index(), left_on='datetime', right_on='Datum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeitwerte werden hinzugefügt\n",
    "df_day['day_of_week'] = df_day['datetime'].dt.dayofweek\n",
    "df_day['season'] =  df_day['datetime'].dt.month%12 // 3 + 1\n",
    "df_day['week_of_year'] =  df_day['datetime'].dt.isocalendar().week\n",
    "df_day['day'] = df_day['datetime'].dt.day\n",
    "df_day['month'] = df_day['datetime'].dt.month\n",
    "df_day['year'] =  df_day['datetime'].dt.year\n",
    "df_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beschriftung der Abbildung mit median min max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buchungen 2011 vs 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily_19 = df_day[df_day['year'] == 2011]\n",
    "df_daily_20 = df_day[df_day['year'] == 2012]\n",
    "df_daily_19.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m = df_day.copy()\n",
    "df_m['quarter'] =df_m['datetime'].dt.quarter\n",
    "df_tt = df_m.groupby(['year','quarter'])[['Bookings']].sum()\n",
    "df_tt=df_tt.iloc[::,:1]\n",
    "df_tt = df_tt.reset_index()\n",
    "df_tt.columns = ['year', 'quarter', 'Bookings']\n",
    "# df_tt['vorquartal'] = df_tt.Bookings.shift()\n",
    "# df_tt['wachstum'] = df_tt['Bookings']/df_tt['vorquartal']\n",
    "# df_tt_2011 =df_tt[df_tt['year']==2011]\n",
    "# df_tt_2012 =df_tt[df_tt['year']==2012]\n",
    "df_tt_2011 =df_tt[df_tt['year']==2011]\n",
    "df_tt_2011 = df_tt_2011[['quarter', 'Bookings']]\n",
    "df_tt_2011.columns = ['quarter', 'Bookings_2011']\n",
    "df_ttt = pd.merge(df_tt, df_tt_2011, on = 'quarter')\n",
    "# df_ttt = df_tt_2011.sort_values()\n",
    "df_tt_2012 =df_ttt[df_ttt['year']==2012]\n",
    "df_tt_2012['Änderung'] = df_ttt['Bookings'] / df_ttt['Bookings_2011']\n",
    "px.bar(df_tt_2012, x = 'quarter', y = 'Änderung')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m = df_day.copy()\n",
    "df_m['quarter'] =df_m['datetime'].dt.quarter\n",
    "df_tt = df_m.groupby(['year'])[['Bookings']].sum()\n",
    "df_tt = df_tt.reset_index()\n",
    "df_tt['Vorjahr'] = df_tt.Bookings.shift()\n",
    "df_tt['Änderung'] = df_tt['Bookings'] / df_tt['Vorjahr']\n",
    "print(f\"Die Änderungsrate zwischen den Jahren beträgt : {df_tt.iloc[1]['Änderung']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(df_tt, x = 'quarter',y = 'wachstum', color = 'year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_month_19 = df_daily_19[['month','Bookings']].groupby('month').sum()\n",
    "df_month_20 = df_daily_20[['month','Bookings']].groupby('month').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Umbennenung Legende und Y-Achse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_month_19.reset_index(), x='month', y=['Bookings',df_month_20['Bookings']], title = 'Vergleich der Buchungen 2011 und 2012 auf Monatsbasis')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram Count -> skewed towards right -> Normalisation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# px histonorm = 'probability density' \n",
    "# Aussage Diagramm ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "sns.displot(df_hourly['Bookings'], kde = True)\n",
    "plt.show()\n",
    "# Normalisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse der Wetterdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_year = df.resample('1Y', on='datetime')[['temp','atemp','humidity','windspeed']].agg(['mean','min','max'])\n",
    "df_year.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=[12,10])\n",
    "ax1 = fig.add_subplot(2,2,1)\n",
    "ax1 = sns.displot(df.temp,bins=range(int(df.temp.min()),int(df.temp.max())),kde=True)\n",
    "\n",
    "\n",
    "ax2 = fig.add_subplot(2,2,2)\n",
    "ax2 = sns.displot(df.atemp,bins=range(int(df.atemp.min()),int(df.atemp.max())),kde=True)\n",
    "\n",
    "\n",
    "ax3 = fig.add_subplot(2,2,3)\n",
    "ax3 = sns.displot(df.humidity,bins=range(int(df.humidity.min()),int(df.humidity.max())),kde=True)\n",
    "\n",
    "\n",
    "ax4 = fig.add_subplot(2,2,4)\n",
    "ax4 = sns.displot(df.windspeed,bins=range(int(df.windspeed.min()),int(df.windspeed.max())),kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count = df_hourly['Bookings'].groupby(df_hourly['weather']).mean()\n",
    "df_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verteilung der Buchung über die einzelnen Wetterkategorien\n",
    "\n",
    "df_weather_cat_sum = df[['datetime','weather']].groupby('weather')['datetime'].count().reset_index(name='Bookings')\n",
    "df_weather_cat_sum['%'] = df_weather_cat_sum['Bookings'] / len(df) * 100\n",
    "df_weather_cat_sum['weather_name'] = df_weather_cat_sum['weather'].map({0:'clear, few clouds',1:'cloudy, mist',2:'light snow or rain or thunderstorm',3:'heavy rain or thunderstorm or snow or ice pallets'})\n",
    "df_weather_cat_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nur 3 Stunden im Jahr bei denen es stark regnet oder stürmt\n",
    "df_w_2 = df_hourly[df_hourly['weather'] == 3]\n",
    "df_w_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12,10])\n",
    "ax1 = fig.add_subplot(2,1,1)\n",
    "ax1 = sns.pointplot(x='month',y='Bookings',hue='weather',data=df_hourly.groupby(['weather','month'])['Bookings'].mean().reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3,ax4) = plt.subplots(ncols=4)\n",
    "fig.set_size_inches(12, 5)\n",
    "sns.regplot(x=\"temp\", y=\"Bookings\", data=df_day,ax=ax1)\n",
    "sns.regplot(x=\"atemp\", y=\"Bookings\", data=df_day,ax=ax2)\n",
    "sns.regplot(x=\"windspeed\", y=\"Bookings\", data=df_day,ax=ax3)\n",
    "sns.regplot(x=\"humidity\", y=\"Bookings\", data=df_day,ax=ax4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_hourly.copy()\n",
    "df_temp.reset_index(inplace=True)\n",
    "df_temp['bin_temp'] = pd.cut(df_temp['temp'], [0, 5,10,15,20,25,30,35,40,45])\n",
    "plt.figure(figsize=(8,4))\n",
    "ax = sns.barplot(x=\"bin_temp\", y=\"Bookings\", data=df_temp,\n",
    "                 palette=\"flare\")\n",
    "ax.set(xlabel='Temperatur in °C', ylabel='Anzahl der Buchungen', title='Übersicht Anzahl der Buchungen je Temperatur')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp[df_temp['temp'] >= 40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wetterdatenauswertung auf die einzelnen Jahre bezogen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registrierte vs nicht registrierte User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customers overall\n",
    "sum_of_cust = df['registered_customer'].count()\n",
    "sum_of_cust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview\n",
    "df.registered_customer.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of registered customers\n",
    "number_reg_cust = df.registered_customer[df.registered_customer==True].count()\n",
    "number_reg_cust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of not registered customers\n",
    "number_N_reg_cust = df.registered_customer[df.registered_customer==False].count()\n",
    "number_N_reg_cust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio registered to all customers\n",
    "ratio_reg_cust = number_reg_cust / sum_of_cust\n",
    "ratio_reg_cust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ratio not registered to all customers\n",
    "ratio_N_reg_cust = number_N_reg_cust / sum_of_cust\n",
    "ratio_N_reg_cust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily overview registered_customer\n",
    "df_registered = df_day[['Bookings', 'registered_customer']]\n",
    "df_registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculation registration_rate per day\n",
    "df_customers_d = df_registered.copy()\n",
    "df_customers_d['unregistered_customers'] = df_day['Bookings']-df_day['registered_customer']\n",
    "df_customers_d['registration_rate'] = df_day['registered_customer'] / df_customers_d['Bookings']\n",
    "df_customers_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monthly overview registered_customer\n",
    "df_month = df_day.groupby(['year','month'])[['Bookings', 'registered_customer']].agg({'Bookings':'sum','registered_customer':'sum'}).reset_index()\n",
    "df_month.sort_values(['year', 'month'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculation registration_rate per month\n",
    "df_customers_m = df_month.copy()\n",
    "df_customers_m['unregistered_customers'] = df_month['Bookings']-df_month['registered_customer']\n",
    "df_customers_m['registration_rate'] = df_month['registered_customer'] / df_customers_m['Bookings']\n",
    "df_customers_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quarter overview registered_customer\n",
    "df_season = df_day.groupby(['year','season'])[['Bookings', 'registered_customer']].agg({'Bookings':'sum','registered_customer':'sum'}).reset_index()\n",
    "df_season.sort_values(['year', 'season'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers_q = df_season.copy()\n",
    "df_customers_q['unregistered_customers'] = df_season['Bookings']-df_season['registered_customer']\n",
    "df_customers_q['registration_rate'] = df_season['registered_customer'] / df_customers_q['Bookings']\n",
    "df_customers_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year overview registered_customer\n",
    "df_year = df_day.groupby(['year'])[['Bookings', 'registered_customer']].agg({'Bookings':'sum','registered_customer':'sum'}).reset_index()\n",
    "df_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers_y = df_year.copy()\n",
    "df_customers_y['unregistered_customers'] = df_year['Bookings']-df_year['registered_customer']\n",
    "df_customers_y['registration_rate'] = df_year['registered_customer'] / df_customers_y['Bookings']\n",
    "df_customers_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_holiday = df_day.groupby(['year', 'holiday'])[['Bookings', 'registered_customer']].agg({'Bookings':'sum','registered_customer':'sum'}).reset_index()\n",
    "df_holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculation registration_rate holiday\n",
    "df_customers_h = df_holiday.copy()\n",
    "df_customers_h['unregistered_customers'] = df_holiday['Bookings']-df_holiday['registered_customer']\n",
    "df_customers_h['registration_rate'] = df_holiday['registered_customer'] / df_customers_h['Bookings']\n",
    "df_customers_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_workingday = df_day.groupby(['year', 'workingday'])[['Bookings', 'registered_customer']].agg({'Bookings':'sum','registered_customer':'sum'}).reset_index()\n",
    "df_workingday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculation registration_rate workingday\n",
    "df_customers_w = df_workingday.copy()\n",
    "df_customers_w['unregistered_customers'] = df_workingday['Bookings']-df_workingday['registered_customer']\n",
    "df_customers_w['registration_rate'] = df_workingday['registered_customer'] / df_customers_w['Bookings']\n",
    "df_customers_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Jahr'] = df.datetime.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['Jahr'], df['registered_customer'], margins=True, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df, x='registered_customer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tag'] = df.datetime.dt.day_name()\n",
    "px.histogram(df, x='tag', histnorm='probability density', color='registered_customer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hourly.groupby(['workingday', 'hour'])[['Bookings']].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hourly.groupby(['day_of_week', 'hour'])[['Bookings']].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hourly.groupby(['day_of_week', 'hour'])[['Bookings']].mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots of average count across hour in a day for various categories\n",
    "\n",
    "f, axes = plt.subplots(nrows=2, ncols=1, figsize=(15, 18))\n",
    "group_work_hour = df_hourly.groupby(['workingday', 'hour'])[['Bookings']].mean().reset_index()\n",
    "sns.pointplot(data=group_work_hour, x='hour', y='Bookings', hue='workingday', ax=axes[0], legend=True)\n",
    "handles, _ = axes[0].get_legend_handles_labels()\n",
    "axes[0].legend(handles, ['Kein Arbeitstag', 'Arbeitstag'])\n",
    "axes[0].set(xlabel='Stunde des Tages', ylabel='Durchschnittliche Bookings', title='Durchschnittliche Bookings pro Stunde abhängig vom Arbeitstag')\n",
    "\n",
    "hue_order= ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "group_day_hour = df_hourly.groupby(['weekday', 'hour'])[['Bookings']].mean().reset_index()\n",
    "sns.pointplot(data=group_day_hour, x='hour', y='Bookings', hue='weekday', ax=axes[1], hue_order=hue_order)\n",
    "axes[1].set(xlabel='Stunde des Tages', ylabel='Durchschnittliche Bookings', title='Durchschnittliche Bookings pro Stunde abhängig vom Wochentag')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_work_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz_h = df.groupby(['hour', 'registered_customer'], as_index=False)[['temp']].count()\n",
    "df_viz_h.columns = ['hour', 'registered_customer', 'Bookings']\n",
    "df_viz_h.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily driving profile registered customers vs. not registered customers\n",
    "fig_h = px.line(df_viz_h, x='hour', y='Bookings', color='registered_customer', title=\" Nutzung registrierte vs. nicht registrierte Kunden auf Stundenbasis\")\n",
    "fig_h.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['kalenderwoche'] = df.datetime.dt.isocalendar().week\n",
    "df_viz_cw2 = df.groupby(['Jahr','kalenderwoche', 'registered_customer'], as_index=False)[['temp']].count()\n",
    "df_viz_cw2.columns = ['Jahr','kalenderwoche', 'registered_customer', 'Bookings']\n",
    "df_viz_cw2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz_cw2['cw_year'] = df_viz_cw2['kalenderwoche'].astype(str) + ' / ' + df_viz_cw2['Jahr'].astype(str)\n",
    "df_viz_cw2 = df_viz_cw2.sort_values(['Jahr','kalenderwoche'], ascending=True).reset_index(drop=True)\n",
    "df_viz_cw2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_cw2 = px.line(df_viz_cw2, x='cw_year',y='Bookings', color='registered_customer', title=\" Nutzung registrierte vs. nicht registrierte Kunden pro Kalenderwoche\")\n",
    "fig_cw2.show()\n",
    "# zu sehen, dass inbesondere bei den Ferientagen ein Einbruch registrierter Kunden stattfindet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workaround um Wochentage in richtige Reihenfolge zu haben\n",
    "sorter = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "sorterIndex = dict(zip(sorter,range(len(sorter))))\n",
    "sorterIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day_id'] = df.index\n",
    "df['day_id'] = df['tag'].map(sorterIndex)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzeige nach Wochentag über Season, Betrachtung saisonaler Einflüsse\n",
    "season_dict = { 1 : \"Winter\", 2 : \"Spring\", 3 : \"Summer\", 4 :\"Autumn\"}\n",
    "df[\"season\"] = df[\"datetime\"].dt.month % 12 // 3 + 1\n",
    "df[\"season\"] =  df[\"season\"].map(season_dict)\n",
    "df_viz_day_season = df.groupby(['season','tag', 'day_id', 'registered_customer'], as_index=False)[['temp']].count()\n",
    "df_viz_day_season.columns = ['season','tag', 'day_id', 'registered_customer', 'Bookings']\n",
    "df_viz_day_season.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz_day_season['day_season'] = df_viz_day_season['tag'].astype(str) + ' / ' + df_viz_day_season['season'].astype(str)\n",
    "df_viz_day_season = df_viz_day_season.sort_values(['season','day_id'], ascending=True).reset_index(drop=True)\n",
    "df_viz_day_season.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_day_season = px.line(df_viz_day_season, x='day_season',y='Bookings', color='registered_customer', title=\" Nutzung registrierte vs. nicht registrierte Kunden pro Wochentag je Jahreszeit\")\n",
    "fig_day_season.show()\n",
    "# bei registrierten Nutzern sind im Winter und Frühling insbesondere Mittwochs stärkere Rückgänge zu sehen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse der bedingten Häufigkeiten\n",
    "# Analyse mittels dem chi2 Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import chi2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abhängigkeit zwischen Jahr und Registered_customer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_k = df.copy()\n",
    "df_k['Hour'] = df_k.datetime.dt.hour\n",
    "df_k['Jahr'] = df_k.datetime.dt.year\n",
    "table = pd.crosstab(df_k['registered_customer'], df_k['Jahr'])\n",
    "table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel = pd.crosstab(df_k['registered_customer'], df_k['Jahr'], margins=True, normalize=True).round(2)\n",
    "rel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch2, p, dof, expected = chi2_contingency(table)\n",
    "# Interpretation des Ergebnisses\n",
    "alpha = 0.05 # Standard level for \"significant\"\n",
    "prob = 1-alpha\n",
    "critical = chi2.ppf(prob, dof) # liefert das <prob>-Quantil der chi2-Verteilung mit <dof> Freiheitsgraden\n",
    "print(f'Bei einer Signifikanz von alpha={alpha}, ist die Testentscheidung:')\n",
    "if ch2 >= critical:\n",
    "\tprint(f'Abhängig (H0 kann wiederlegt werden)')\n",
    "else:\n",
    "\tprint('Unabhängig (fail to reject H0)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jahr 2011 oder 2012\n",
    "# Registered_customer : 1 = Benutzer ist registiert\n",
    "#                       0 = Benutzer ist nicht registriert\n",
    "P2019_1 = rel.reset_index().iloc[1][2011]\n",
    "P2019 = rel.reset_index().iloc[2][2011]\n",
    "P1bed2019 =  P2019_1 / P2019\n",
    "P2020_1 = rel.reset_index().iloc[1][2012]\n",
    "P2020 = rel.reset_index().iloc[2][2012]\n",
    "P1bed2020 =  P2020_1 / P2020\n",
    "print(f'Wahrscheinlichkeit für P(1|2011) = {P1bed2019}')\n",
    "print(f'Wahrscheinlichkeit für P(1|2012) = {P1bed2020}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da die bedingte Wahrscheinlichkeit bei einem Zufallsexperiment eine registrierte Person zu ziehen ist abhängig von dem Jahr. Unter der Bedingung, dass das Jahr bekannt ist verändert sich die Wahrscheinlichkeit. Außerdem kann hier erkannt werden, dass die Wahrscheinlichkeit für eine registrierte Person 2012 höher ist als 2011. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abhängigkeit zwischen registered_customer und weather\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.crosstab(df_k['registered_customer'], df_k['weather'])\n",
    "table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel = pd.crosstab(df_k['registered_customer'], df_k['weather'], margins=True, normalize=True)\n",
    "rel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch2, p, dof, expected = chi2_contingency(table)\n",
    "# Interpretation des Ergebnisses\n",
    "alpha = 0.05 # Standard level for \"significant\"\n",
    "prob = 1-alpha\n",
    "critical = chi2.ppf(prob, dof) # liefert das <prob>-Quantil der chi2-Verteilung mit <dof> Freiheitsgraden\n",
    "print(f'Bei einer Signifikanz von alpha={alpha}, ist die Testentscheidung:')\n",
    "if ch2 >= critical:\n",
    "\tprint(f'Abhängig (H0 kann wiederlegt werden)')\n",
    "else:\n",
    "\tprint('Unabhängig (fail to reject H0)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather : 1=clear, 2=cloudy, 3=snow, 4=heavy_rain\n",
    "# Registered_customer : 1 = Benutzer ist registiert\n",
    "#                       0 = Benutzer ist nicht registriert\n",
    "Pclear_1 = rel.reset_index().iloc[1][0]\n",
    "Pclear = rel.reset_index().iloc[2][0]\n",
    "P1bedclear =  Pclear_1 / Pclear\n",
    "Pcloudy_1 = rel.reset_index().iloc[1][1]\n",
    "Pcloudy = rel.reset_index().iloc[2][1]\n",
    "P1bedcloudy =  Pcloudy_1 / Pcloudy\n",
    "Pheavy_1 = rel.reset_index().iloc[1][3]\n",
    "Pheavy = rel.reset_index().iloc[2][3]\n",
    "P1bedheavy =  Pheavy_1 / Pheavy\n",
    "Psnow_1 = rel.reset_index().iloc[1][2]\n",
    "Psnow = rel.reset_index().iloc[2][2]\n",
    "P1bedsnow =  Psnow_1 / Psnow\n",
    "\n",
    "print(f'Wahrscheinlichkeit für P(1|clear) = {P1bedclear}')\n",
    "print(f'Wahrscheinlichkeit für P(1|cloudy) = {P1bedcloudy}')\n",
    "print(f'Wahrscheinlichkeit für P(1|heavy) = {P1bedheavy}')\n",
    "print(f'Wahrscheinlichkeit für P(1|snow) = {P1bedsnow}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier kann erkannt werden, dass gerade bei schlechtem Wetter die Wahrscheinlichkeit für eine registrierte Person steigt, beispielsweise sind bei der Wetterkategorie 'heavy rain or thunderstorm or snow or ice pallets' 96.25% der Kunden registriert.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abhängigkeit zwischen registered_customer und workingday\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.crosstab(df_k['registered_customer'], df_k['workingday'])\n",
    "table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel = pd.crosstab(df_k['registered_customer'], df_k['workingday'], margins=True, normalize=True)\n",
    "rel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch2, p, dof, expected = chi2_contingency(table)\n",
    "# Interpretation des Ergebnisses\n",
    "alpha = 0.05 # Standard level for \"significant\"\n",
    "prob = 1-alpha\n",
    "critical = chi2.ppf(prob, dof) # liefert das <prob>-Quantil der chi2-Verteilung mit <dof> Freiheitsgraden\n",
    "print(f'Bei einer Signifikanz von alpha={alpha}, ist die Testentscheidung:')\n",
    "if ch2 >= critical:\n",
    "\tprint(f'Abhängig (H0 kann wiederlegt werden)')\n",
    "else:\n",
    "\tprint('Unabhängig (H0 kann nicht wiederlegt werden)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workingday : y / n\n",
    "# Registered_customer : 1 = Benutzer ist registiert\n",
    "#                       0 = Benutzer ist nicht registriert\n",
    "Pn_1 = rel.reset_index().iloc[1][0]\n",
    "Pn = rel.reset_index().iloc[2][0]\n",
    "P1bedn =  Pn_1 / Pn\n",
    "Py_1 = rel.reset_index().iloc[1][1]\n",
    "Py = rel.reset_index().iloc[2][1]\n",
    "P1bedy =  Py_1 / Py\n",
    "\n",
    "\n",
    "print(f'Wahrscheinlichkeit für P(1|no) = {P1bedn}')\n",
    "print(f'Wahrscheinlichkeit für P(1|yes) = {P1bedy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unter der Bedingung, dass es kein Arbeitstag ist, sinkt auch die Wahrscheinlichkeit für die registrierten Kunden, da nurnoch 68.7% der Kunden registriert sind. Prüft man wiederum in einem Zufallsexperiment an einem Arbeitstag ob es sich um einen registrierten Kunden handelt, so steigt die Wahrscheinlichkeit, das es sich um einen Registrierten Kunden handelt auf 86.9%. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Abhängigkeit zwischen registered_customer und holiday\n",
    "table = pd.crosstab(df_k['registered_customer'], df_k['holiday'])\n",
    "table\n",
    "rel = pd.crosstab(df_k['registered_customer'], df_k['holiday'], margins=True, normalize=True)\n",
    "rel\n",
    "ch2, p, dof, expected = chi2_contingency(table)\n",
    "# Interpretation des Ergebnisses\n",
    "alpha = 0.05 # Standard level for \"significant\"\n",
    "prob = 1-alpha\n",
    "critical = chi2.ppf(prob, dof) # liefert das <prob>-Quantil der chi2-Verteilung mit <dof> Freiheitsgraden\n",
    "print(f'Bei einer Signifikanz von alpha={alpha}, ist die Testentscheidung:')\n",
    "if ch2 >= critical:\n",
    "\tprint(f'Abhängig (H0 kann wiederlegt werden)')\n",
    "else:\n",
    "\tprint('Unabhängig (H0 kann nicht wiederlegt werden)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holiday : y / n\n",
    "# Registered_customer : 1 = Benutzer ist registiert\n",
    "#                       0 = Benutzer ist nicht registriert\n",
    "Pn_1 = rel.reset_index().iloc[1][0]\n",
    "Pn = rel.reset_index().iloc[2][0]\n",
    "P1bedn =  Pn_1 / Pn\n",
    "Py_1 = rel.reset_index().iloc[1][1]\n",
    "Py = rel.reset_index().iloc[2][1]\n",
    "P1bedy =  Py_1 / Py\n",
    "\n",
    "\n",
    "print(f'Wahrscheinlichkeit für P(1|no) = {P1bedn}')\n",
    "print(f'Wahrscheinlichkeit für P(1|yes) = {P1bedy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Wahrscheinlichkeit bei einem Zufallexperiment für eine Person welche registriert ist, ist an einem Tag an dem keine Ferien ist 81.47% und damit größer als an einem Tag an dem Ferien sind, da hier die Wahrscheinlichkeit nur 74.5% beträgt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abhängigkeit zwischen workingday und weather \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.crosstab(df_k['workingday'], df_k['weather'])\n",
    "table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel = pd.crosstab(df_k['workingday'], df_k['weather'], margins=True, normalize=True)\n",
    "rel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch2, p, dof, expected = chi2_contingency(table)\n",
    "# Interpretation des Ergebnisses\n",
    "alpha = 0.05 # Standard level for \"significant\"\n",
    "prob = 1-alpha\n",
    "critical = chi2.ppf(prob, dof) # liefert das <prob>-Quantil der chi2-Verteilung mit <dof> Freiheitsgraden\n",
    "print(f'Bei einer Signifikanz von alpha={alpha}, ist die Testentscheidung:')\n",
    "if ch2 >= critical:\n",
    "\tprint(f'Abhängig (H0 kann wiederlegt werden)')\n",
    "else:\n",
    "\tprint('Unabhängig (H0 kann nicht wiederlegt werden)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather : clear, cloudy, heavy_rain, snow\n",
    "# Workingday :          1 = Tag ist ein Arbeitstag\n",
    "#                       0 = Tag ist kein Arbeitstag\n",
    "Pclear_1 = rel.reset_index().iloc[1][0]\n",
    "Pclear = rel.reset_index().iloc[2][0]\n",
    "P1bedclear =  Pclear_1 / Pclear\n",
    "Pcloudy_1 = rel.reset_index().iloc[1][1]\n",
    "Pcloudy = rel.reset_index().iloc[2][1]\n",
    "P1bedcloudy =  Pcloudy_1 / Pcloudy\n",
    "Pheavy_1 = rel.reset_index().iloc[1][3]\n",
    "Pheavy = rel.reset_index().iloc[2][3]\n",
    "P1bedheavy =  Pheavy_1 / Pheavy\n",
    "Psnow_1 = rel.reset_index().iloc[1][2]\n",
    "Psnow = rel.reset_index().iloc[2][2]\n",
    "P1bedsnow =  Psnow_1 / Psnow\n",
    "\n",
    "print(f'Wahrscheinlichkeit für P(1|clear) = {P1bedclear}')\n",
    "print(f'Wahrscheinlichkeit für P(1|cloudy) = {P1bedcloudy}')\n",
    "print(f'Wahrscheinlichkeit für P(1|heavy) = {P1bedheavy}')\n",
    "print(f'Wahrscheinlichkeit für P(1|snow) = {P1bedsnow}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei diesen bedingten Häufigkeiten muss aufgepasst werden, dass nicht von einer Kausalen Abhängigkeit ausgegangen werden kann, da aus logischer Sicht keine Beeinflussung des Wetters und des Arbeitstags vorliegt. Der Zusammehang geht zwar aus den Zahlen hervor, allerdings ist der Wirkungszusammenhang in der Realität eher fragwürdig. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abhängigkeit zwischen workingday und weather unter der Bedingung das registered_customer bekannt ist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_k.registered_customer == 1\n",
    "df_km = df_k[mask]\n",
    "table = pd.crosstab(df_km['workingday'], df_km['weather'])\n",
    "table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel = pd.crosstab(df_km['workingday'], df_km['weather'], margins=True, normalize=True)\n",
    "rel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch2, p, dof, expected = chi2_contingency(table)\n",
    "# Interpretation des Ergebnisses\n",
    "alpha = 0.05 # Standard level for \"significant\"\n",
    "prob = 1-alpha\n",
    "critical = chi2.ppf(prob, dof) # liefert das <prob>-Quantil der chi2-Verteilung mit <dof> Freiheitsgraden\n",
    "print(f'Bei einer Signifikanz von alpha={alpha}, ist die Testentscheidung:')\n",
    "if ch2 >= critical:\n",
    "\tprint(f'Abhängig (H0 kann wiederlegt werden)')\n",
    "else:\n",
    "\tprint('Unabhängig (H0 kann nicht wiederlegt werden)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather : clear, cloudy, heavy_rain, snow\n",
    "# Workingday :          1 = Tag ist ein Arbeitstag\n",
    "#                       0 = Tag ist kein Arbeitstag\n",
    "Pclear_1 = rel.reset_index().iloc[1][0]\n",
    "Pclear = rel.reset_index().iloc[2][0]\n",
    "P1bedclear =  Pclear_1 / Pclear\n",
    "Pcloudy_1 = rel.reset_index().iloc[1][1]\n",
    "Pcloudy = rel.reset_index().iloc[2][1]\n",
    "P1bedcloudy =  Pcloudy_1 / Pcloudy\n",
    "Pheavy_1 = rel.reset_index().iloc[1][3]\n",
    "Pheavy = rel.reset_index().iloc[2][3]\n",
    "P1bedheavy =  Pheavy_1 / Pheavy\n",
    "Psnow_1 = rel.reset_index().iloc[1][2]\n",
    "Psnow = rel.reset_index().iloc[2][2]\n",
    "P1bedsnow =  Psnow_1 / Psnow\n",
    "\n",
    "print(f'Wahrscheinlichkeit für P(1|clear) = {P1bedclear}')\n",
    "print(f'Wahrscheinlichkeit für P(1|cloudy) = {P1bedcloudy}')\n",
    "print(f'Wahrscheinlichkeit für P(1|heavy) = {P1bedheavy}')\n",
    "print(f'Wahrscheinlichkeit für P(1|snow) = {P1bedsnow}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somit wurde auf folgendes Modell geprüft:\n",
    "weahter -> registrierten_kunden <- workingday\n",
    "Da allerdings weather und workingday im Datensatz eine abhängigkeit haben unabhängig ob die Kunden registriert sind oder nicht muss das Modell wiederlegt werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welchen Einfluss hat das Wetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nutzungsverhalten in Zeit Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_overall_count = px.line(df_day.reset_index(),y='Bookings', x=\"datetime\", title = 'Anzahl der Buchungen 2011-2012')\n",
    "fig_overall_count.update_xaxes(\n",
    "    rangeslider_visible=True,\n",
    "    rangeselector=dict(\n",
    "        buttons=list([\n",
    "            dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n",
    "            dict(count=3, label=\"3m\", step=\"month\", stepmode=\"backward\"),\n",
    "            dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n",
    "            dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n",
    "            dict(step=\"all\")])))\n",
    "fig_overall_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_VizSeasons=pd.DataFrame(df_hourly.groupby('season').sum()['Bookings'].sort_values(ascending=False)).reset_index()\n",
    "df_VizSeasons.style.background_gradient(cmap=sns.light_palette(\"green\", as_cmap=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_season = df_hourly.groupby(\"season\")[\"Bookings\"].sum().reset_index(name=\"rented_bikes\")\n",
    "fig_pie = px.pie(df_by_season,values=\"rented_bikes\",names=\"season\", title=\"Anzahl der ausgeliehenen E-Scooter pro Jahreszeit\")\n",
    "fig_pie.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_sum = df_hourly.groupby(df_hourly[\"hour\"]).agg({\"Bookings\": \"sum\"})\n",
    "fig = px.bar(hourly_sum, title=\"Stündliche Verteilung der Buchungen innerhalb 2 Jahren\", labels= {\"value\" : \"Gesamtanzahl der Buchungen pro Stunde\", \"datetime\": \"Uhrzeit in Stunde\"})\n",
    "fig.update_layout(showlegend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_avg_per_day_all = df.groupby([df[\"datetime\"].dt.isocalendar().year,df[\"datetime\"].dt.isocalendar().week,df[\"datetime\"].dt.isocalendar().day])[\"registered_customer\"].sum().reset_index(name=\"sum_of_rentals_per_day\")\n",
    "df_avg_per_day_all  = df_avg_per_day_all.groupby([\"year\",\"week\"])[\"sum_of_rentals_per_day\"].mean().reset_index(name=\"avg_per_wk\")\n",
    "px.line(df_avg_per_day_all,x=\"week\",y=\"avg_per_wk\",color=\"year\",markers=True, title='Durchschnittliche Buchungen pro Woche für 2011 und 2012',\n",
    "        labels = {'avg_per_wk' : 'Durchschnitt pro Woche'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_per_day_all.groupby(\"year\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_index = df.set_index('datetime')\n",
    "df_viz = df_time_index.groupby([pd.Grouper(freq='1W')]).agg({'registered_customer':'count'}).reset_index()\n",
    "fig = px.line(df_viz,x=\"datetime\",y=\"registered_customer\",color=df_viz[\"datetime\"].dt.year)\n",
    "# fig.update_layout(legend = {'2011' : '2011', '2012' : '2012'}) \n",
    "# legende Bearbeiten!\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_week = df_day[(df_day[\"day_of_week\"] != 5) & (df_day[\"day_of_week\"] != 6)]\n",
    "df_viz_pW = df_week.groupby(\"day_of_week\")[\"Bookings\"].sum()\n",
    "px.bar(df_viz_pW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekend = df_day[~((df_day[\"day_of_week\"] != 5) & (df_day[\"day_of_week\"] != 6))]\n",
    "df_viz_WeEnd = df_weekend.groupby(\"day_of_week\")[\"Bookings\"].sum()\n",
    "px.bar(df_viz_WeEnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vizT = df_hourly.groupby([\"temp\"])[[\"Bookings\"]].sum().reset_index()\n",
    "figT = px.histogram(df_vizT, x=\"temp\", y=\"Bookings\", nbins=15 )\n",
    "figT.update_layout(showlegend=False,bargap=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vizAt = df_hourly.groupby([\"atemp\"])[[\"Bookings\"]].sum().reset_index()\n",
    "figAt = px.histogram(df_vizAt, x=\"atemp\", y=\"Bookings\", nbins=15 )\n",
    "figAt.update_layout(showlegend=False,bargap=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Korrelationsmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrMatt = df_day[['holiday', 'workingday', 'temp', 'atemp', 'humidity', 'windspeed', 'registered_customer', 'unregistered_customer','Bookings', 'year',\n",
    "                   'season', 'weather', 'week_of_year']].corr()\n",
    "mask = np.array(corrMatt)\n",
    "mask[np.tril_indices_from(mask)] = False\n",
    "# fig,ax= plt.subplots()\n",
    "# fig.set_size_inches(20,10)\n",
    "sns.set(rc = {'figure.figsize' : (20,10)})\n",
    "sns.heatmap(corrMatt, annot=True, cmap='coolwarm', mask = mask).set(title='Korrelationsmatrix über ausgwählte Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prognose\n",
    "## Zeitreihenanalyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample des DF auf den Monat\n",
    "df_time = df.set_index('datetime') \n",
    "df_time.index = pd.to_datetime(df_time.index) # Sichergehen, dass Index das richtige Format hat. \n",
    "df_time = df_time.resample('M').count()[['temp']]\n",
    "df_time.columns = ['Bookings']\n",
    "df_time['Vorheriger_Monat'] = df_time['Bookings'].shift()\n",
    "\n",
    "# Einfügen von Mulitplikativen und Additiven Unterschieden\n",
    "df_time['Mul'] = df_time['Bookings'] / df_time['Vorheriger_Monat']\n",
    "df_time['Add'] = df_time['Bookings'] - df_time['Vorheriger_Monat']\n",
    "\n",
    "# Einfügen des Rolling Average\n",
    "df_time['Rol_avg'] = df_time['Bookings'].rolling(window = 3).mean().round(2)\n",
    "df_time['Monat_t'] = range(1,25)\n",
    "df_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine weitere Möglichkeit zur Analyse der Daten in Bezug auf die Bookings ist das Betrachten als Zeitreihe. Hier werden die Bookings über die 2 Jahre hinweg Beobachtet und die Entwicklung der Größe analysiert. Die Folgende Grafik zeigt die Zeitliche Entwicklung der Verkäufe, aggregiert auf den Monat und die gleitenden Durchschnitte der Zeitreihe der Ordnung drei. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_time.reset_index(),x = 'datetime', y = ['Bookings', 'Rol_avg'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max von 2011 : {df_time.loc['2011']['Bookings'].idxmax()} mit {df_time.loc['2011']['Bookings'].max()} Bookings\")\n",
    "print(f\"Min von 2011 : {df_time.loc['2011']['Bookings'].idxmin()} mit {df_time.loc['2011']['Bookings'].min()} Bookings\")\n",
    "print(f\"Max von 2012 : {df_time.loc['2012']['Bookings'].idxmax()} mit {df_time.loc['2012']['Bookings'].max()} Bookings\")\n",
    "print(f\"Min von 2012 : {df_time.loc['2012']['Bookings'].idxmin()} mit {df_time.loc['2012']['Bookings'].min()} Bookings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auf beiden Kurven kann ein positiver Trend gesehen werden, jedoch können auch Saisonale Einflüsse erkannt werden, welche je nach Monat die Verkaufszahlen beeinflussen. In den Jahren 2011 und 2012 war der Monat Januar jeweils der Monat mit den meisten Verkäufen, während 2011 im Juli und 2012 im Semptember die Menge der Bookings am größten war. Allerdings konnten selbst im Januar 2012  die Bookings um 255%(56588) im Vergleich zum Januar 2011 gesteigert werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden werden die Additiven und die Multiplikativen Veränderungen der Zeitreihe zwischen den Monaten angeschaut, die Rote Linie in den Diagrammen signalisiert hierbei ob der absolute Wert größer oder kleiner als im voerherigen Monat ist, abhängig ob der Wert auf dem Diagramm über oder unter der Linie ist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_time.reset_index(), x = 'datetime', y = 'Mul', title='Relative Veränderung von Bookings pro Monat')\n",
    "fig.add_shape(type = 'line', xref='paper', x0=0, x1=12, opacity=0.5, line_color = 'red', \n",
    "              y0=1, y1=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_time.reset_index(), x = 'datetime', y = 'Add', title='Additive Veränderung von Bookings pro Monat')\n",
    "fig.add_shape(type = 'line', xref='paper', x0=0, x1=12, opacity=0.5, line_color = 'red', \n",
    "              y0=0, y1=0)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ermittlung der Regressionsgerade\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = df_time[['Monat_t']]\n",
    "y = df_time[['Bookings']]\n",
    "\n",
    "model = LinearRegression(fit_intercept=True) # Verschiebung auf der y-Achse wird zugelassen\n",
    "model.fit(X,y)\n",
    "df_time['Prediction'] = model.predict(X)\n",
    "print('Regressionsfunktion aus Statistik : y = ax + b')\n",
    "print(f\"Koeffizient a = {round(float(model.coef_), 2)}\")\n",
    "print(f\"Koeffizient b = {round(float(model.intercept_), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mittles Sklearn lässt sich die Regressionsgerade für die Bookings ermitteln, dise stellt eine Trendgerade für die Zeitreihe dar. Für die Trendgerade ergibt sich die Gleichung, y = 5760.29 * t + 65330.56, also pro Monat erhöht sich der Trend um 5760.29 Bookings mit einem Anfangswert von 65330.56 Bookings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualierung der Trendgerade und den monatlichen Werten\n",
    "px.line(df_time.reset_index(), x = 'datetime', y = ['Bookings', 'Prediction'], title = 'Bookings pro Monat und die Trendkomponente der Zeitreihe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualierung des Unterschieds zwischen den Trendgeraden und den monatlichen Werten\n",
    "df_time['Unterschied_Prognose'] = df_time['Bookings'] - df_time['Prediction']\n",
    "fig = px.area(df_time.reset_index(), x = 'datetime', y = 'Unterschied_Prognose', title='Unterschied zum Trend')\n",
    "fig.add_shape(type = 'line', xref='paper', x0=0, x1=12, opacity=0.5, line_color = 'red', \n",
    "              y0=0, y1=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auf dem Diagramm können die Schwankungen der Zeitreihe um die Trendgerade beobachtet werden, man erkennt hier, dass die Schwankungen periodisch sind, also in manchen Perioden liegt der eigentliche Wert über dem Trend und in manchen Phasen unter dem Trend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ermittlung der Saisonalen Einflüsse\n",
    "df_time['Monat'] = pd.DatetimeIndex(df_time.index).month # Monat wird aus dem Index extrahiert. \n",
    "\n",
    "# Gruppierung nach dem Monat liefert die durschnittliche Abweichung pro Monat\n",
    "df_time_seasonal = df_time.groupby(['Monat'], as_index=False)[['Unterschied_Prognose']].mean()\n",
    "df_time_seasonal.columns = ['Monat', 'Saisonale_Komponente']\n",
    "\n",
    "# Die durchschnittliche Abweichung wird dem df hinzugefügt\n",
    "df_time_sc = pd.merge(df_time.reset_index(), df_time_seasonal, on = 'Monat').sort_values('datetime')\n",
    "df_time_sc = df_time_sc.set_index('datetime')\n",
    "\n",
    "# durch Addition des Trends mit den monatlichen Werten ergibt sich ein erster Verlauf\n",
    "df_time_sc['Seasonal_Prediction'] = df_time_sc['Prediction'] + df_time_sc['Saisonale_Komponente']\n",
    "df_time_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_time_sc.reset_index(), x = 'datetime', y = ['Bookings','Prediction', 'Seasonal_Prediction'],\n",
    "         title = 'Bookings, Trendgerade und eine erste Vorsage für 2011 und 2012')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorhersage für die zukünftigen Monate\n",
    "### Werden die Saisonalen Werte zu dem jeweiligen Wert der Trendgerade addiert kann eine erste Vorhersage aufgestellt werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellung eines DF welcher das Jahr 2013 enthält\n",
    "df_2013_22 = pd.DataFrame(pd.date_range(start = '2013-01-01', end='2013-12-31', freq='M'))\n",
    "df_2013_22.columns = ['datetime']\n",
    "df_2013_22['Jahr'] = df_2013_22.datetime.dt.year\n",
    "df_2013_22['Monat'] = df_2013_22.datetime.dt.month\n",
    "df_2013_22['Monat_t'] = range(25,37)  \n",
    "\n",
    "# Bildung der Trendgerade\n",
    "df_2013_22['Prediction'] = model.predict(df_2013_22[['Monat_t']])\n",
    "\n",
    "# monatliche Abweichungen werden zum df hinzugefügt und addiert\n",
    "df_2013_22 = pd.merge(df_2013_22, df_time_seasonal, on='Monat').sort_values('datetime')\n",
    "df_2013_22['Seasonal_Prediction'] = df_2013_22['Prediction'] + df_2013_22['Saisonale_Komponente']\n",
    "df_2013_22['datetime'] = pd.to_datetime(df_2013_22['datetime'])\n",
    "df_2013_22 = df_2013_22.set_index('datetime')\n",
    "df_2013_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# die Prognose für das Jahr 2013 wird an den monatlichen DF angeheftet\n",
    "df_viz = df_time_sc[['Bookings', 'Seasonal_Prediction', 'Prediction']]\n",
    "df_viz = pd.concat([df_viz, df_2013_22[['Seasonal_Prediction', 'Prediction']]])\n",
    "df_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Änderung der Formatierung\n",
    "fig = px.line(df_viz.reset_index(), x = 'datetime', y = ['Seasonal_Prediction', 'Prediction', 'Bookings'],\n",
    "        title='Bookings bis 2013 und Trend/Vorhersage bis Ende 2014')\n",
    "\n",
    "# fig.update_layout(plot_bgcolor = \"#fff\", x_axis = 15)\n",
    "# fig.update_traces(line = {'color' : 'Black', 'width' : 2})\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "model1 = ExponentialSmoothing(df_time_sc['Bookings'],\n",
    "                             freq = 'M',\n",
    "                             trend = 'add', \n",
    "                             seasonal = 'add',\n",
    "                             seasonal_periods = 12).fit(smoothing_level = 0.9)\n",
    "\n",
    "model2 = ExponentialSmoothing(df_time_sc['Bookings'],\n",
    "                             freq = 'M',\n",
    "                             trend = 'add', \n",
    "                             seasonal = 'add',\n",
    "                             seasonal_periods = 12).fit(smoothing_level = 0.1)\n",
    "\n",
    "df_time_sc.index = pd.to_datetime(df_time_sc.index)\n",
    "\n",
    "model_autoreg = AutoReg(df_time_sc['Bookings'],None,trend='ct', seasonal=True, period = 12) # period in DF information\n",
    "res = model_autoreg.fit()\n",
    "\n",
    "\n",
    "df_viz['Exponential_Smoothing0.9'] = model1.predict(start = 0, end = 36)\n",
    "df_viz['Exponential_Smoothing0.1'] = model2.predict(start = 0, end = 36)\n",
    "df_viz['AutoReg'] = res.predict(start = 0, end = 36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.area(df_viz.loc[ : '2012'], y = 'Bookings',\n",
    "        title='Bookings bis 2013 und Trend/Vorhersage bis Ende 2014')\n",
    "\n",
    "fig.update_traces(line = {'color' : 'lightGreen', 'width' : 0.3})\n",
    "\n",
    "fig.add_scatter(x = df_viz.reset_index()['datetime'], y = df_viz.reset_index()['Prediction'], line = {'dash' : 'dot'}, name='Trend')\n",
    "fig.add_scatter(x = df_viz.reset_index()['datetime'], y = df_viz.reset_index()['Seasonal_Prediction'], line = {'color' : 'purple'}, name='Prognose' )\n",
    "fig.add_scatter(x = df_viz.reset_index()['datetime'], y = df_viz.reset_index()['Exponential_Smoothing0.9'], line = {'color' : 'blue'}, name='Exponentielle_Glättung_0.9')\n",
    "fig.add_scatter(x = df_viz.reset_index()['datetime'], y = df_viz.reset_index()['Exponential_Smoothing0.1'], line = {'color' : '#2E8B57'}, name='Exponentielle_Glättung_0.1')\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ermittlung des Modells mit geringster Abweichung\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "df_met = df_viz.loc[ : '2012']\n",
    "print(r2_score(df_met['Bookings'], df_met['Seasonal_Prediction'])) # -> am höchsten da es nicht geglättet ist!\n",
    "print(r2_score(df_met['Bookings'], df_met['Exponential_Smoothing0.9']))\n",
    "print(r2_score(df_met['Bookings'], df_met['Exponential_Smoothing0.1']))\n",
    "print(r2_score(df_met['Bookings'], df_met['Exponential_Smoothing0.1']))\n",
    "print(r2_score(df_met['Bookings'], df_met['AutoReg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max von 2013 : {df_viz.loc['2013' : ]['AutoReg'].idxmax()} mit {df_viz.loc['2013' : ]['AutoReg'].max()} Bookings\")\n",
    "print(f\"Min von 2013 : {df_viz.loc['2013' : ]['AutoReg'].idxmin()} mit {df_viz.loc['2013' : ]['AutoReg'].min()} Bookings\")\n",
    "print(f\"Durchschnitt von 2013 : Average = {df_viz.loc['2013' : ]['AutoReg'].mean()} Bookings\")\n",
    "print(f\"Summe von 2013 : Summe = {df_viz.loc['2013' : ]['AutoReg'].sum()} Bookings\")\n",
    "print(f\"Die Prozentuale Veränderung gegenüber 2012 : {round((df_viz.loc['2013' : ]['AutoReg'].sum() / df_viz.loc['2012']['Bookings'].sum() -1 ) * 100,2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Bildung einer Prognose für die zukünftigen Jahre ist dabei möglich durch die lineare Fortschreibung sowie mit der exponentiellen Glättung. Man erhält hier je nach Parameter und Methode verschiedene Prognosewerte für 2013 mit der Annahme, dass die bisherige Entwicklung in der Zukunft weiterläuft. Ein Modell sieht dabei für 2013 den Januar mit 168362 Buchungen als schwächsten Monat und den Juni mit 277493 Buchungen als stärksten Monat. Des Weiteren werden 241019 Buchungen im Schnitt vorhergesagt und für das gesamte Jahr 2892233 Buchungen. Dies entspricht einem Wachstum gegenüber 2012 von 40.53%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tägliche Vorhersage mittels der Regressionsanalyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine Fragestellung, welche sich eher auf das operationale Tagesgeschäft bezieht und dabei die einzelnen Elemente unserer Analyse vereint, ist die Vorhersage der täglichen Nutzung der Scooter und dessen Einflussfaktoren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_day['weather_name']\n",
    "px.scatter(df_day, x = 'atemp', y = 'Bookings', color = 'year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auf dem Scatterplot kann bereits erkannt werden,dass ein Zusammenhang zwischen der gefühlten Temperatur und den Bookings vorliegt. Da die Bookings über die Jahre hinweg zugenommen haben, wird der Zusammenhang noch größer wenn man ihn abhängig vom Jahr betrachtet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Korrelation zwischen Bookings und atemp : {np.corrcoef(df_day['Bookings'], df_day['atemp'])[0,1]}\")\n",
    "mask = (df_day.year == 2011) \n",
    "print(f\"Korrelation zwischen Bookings und atemp im Jahr 2011 : {np.corrcoef(df_day[mask]['Bookings'], df_day[mask]['atemp'])[0,1]}\")\n",
    "mask = df_day.year == 2012\n",
    "print(f\"Korrelation zwischen Bookings und atemp im Jahr 2012 : {np.corrcoef(df_day[mask]['Bookings'], df_day[mask]['atemp'])[0,1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Wird die allgemeine Steigerung der Bookings zwischen 2019 und 2020 beachtet, also dass im Durchschnitt täglich 65.8% mehr gefahren wurde, so ergibt sich eine Korrelation zwischen „atemp“ und „Bookings“ für das Jahr 2011 von 0.79 und für 2012 ergibt sich 0.72. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask = df_day['year'] == 2011\n",
    "px.scatter(df_day[mask], x = 'atemp', y = 'Bookings', color = 'month')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Des Weiteren muss auch die monatliche Steigerung der Bookings beachtet werden, da es einen allgemeinen Trend über das Jahr gibt. Bei Betrachtung des Scatterplots kann man erkennen, dass spätere Monate im Jahr oftmals bei einer selben gefühlten Temperatur eine höhere Zahl an Bookings haben. Betrachtet man beispielsweise den Januar und den Dezember, hat der Dezember oft bei gleicher Temperatur, über viele Werte hinweg, mehr Bookings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_day.groupby(['weather_clear, few clouds'])['Bookings'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day.groupby(['weather_cloudy, mist'])['Bookings'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day.groupby(['weather_light snow or rain or thunderstorm'])['Bookings'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "x = df_day[['atemp', 'year', 'weather_clear, few clouds',    \n",
    "            'weather_light snow or rain or thunderstorm','month']]\n",
    "y = df_day[['Bookings']]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x, y)\n",
    "x = scaler.transform(x)\n",
    "\n",
    "# poly = PolynomialFeatures()\n",
    "# x = poly.fit_transform(x)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y , test_size=0.8, random_state=42)\n",
    "\n",
    "model = Lasso()\n",
    "model.fit(x,y)\n",
    "print(r2_score(y_train, model.predict(X_train)))\n",
    "print(r2_score(y_test, model.predict(X_test)))\n",
    "model.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_coef = pd.DataFrame(model.coef_.reshape(1,-1), columns=['atemp', 'Jahr', 'weather_clear, few clouds', \n",
    "            'weather_light snow or rain or thunderstorm', 'Monat'] )\n",
    "df_coef\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_day['Predict'] = model.predict(x)\n",
    "px.line(df_day, x = 'datetime', y = ['Bookings', 'Predict'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Modell mit besserem r2 Score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Benutzt man bei dem Modell mehr Features wie Beispielsweise Workingday, Windspeed und Humidity wird das Modell zwar komplexer, allerdings wird hier die Annahme getroffen, dass das Modell durch die Erhöhung der Informationen auch genauere Vorhersgaen treffen kann. Außerdem kann mit der Wetterspalte der r2_score erhöht werden, dieser war zuvor Dummy-codiert wobei immer der Wert welcher an dem Tag am meisten vorgekommen ist eine 1 hatte und die anderen Werte eine 0. Mit der Verwendung von einer prozentualen Skalierung bekommt das Modell so mehr Informationen. Das Wetter am 1. Januar wäre zur Interpretation also zu 31.17% klar gewesen, so 61.45% bewölkt und so 7.36% hat es geschneit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_dummy = pd.get_dummies(df[['datetime', 'weather_name']], columns=['weather_name'])\n",
    "df_dummy = df_dummy.groupby(['datetime']).mean()\n",
    "df_dummy.index = pd.to_datetime(df_dummy.index)\n",
    "df_dummy = df_dummy.resample('d').mean()\n",
    "df_dummy = df_dummy.reset_index()\n",
    "df_dummy.columns = ['datetime', 'weather_clear', 'weather_cloudy', 'weather_heavy', 'weather_snow']\n",
    "df_dummy.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_reg = pd.merge(df_day, df_dummy, on = 'datetime')\n",
    "df_reg.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "\n",
    "col = ['atemp', 'humidity','windspeed', 'year',\n",
    "       'workingday', 'weather_clear', 'weather_cloudy', 'weather_heavy', 'weather_snow', 'month']\n",
    "xdf_wm = df_reg[col]\n",
    "xdf_wm = pd.get_dummies(xdf_wm, columns=['month', 'year'])\n",
    "ydf_wm = df_reg[['Bookings']]\n",
    "\n",
    "# Linear Regression\n",
    "X_train, X_test, y_train, y_test = train_test_split(xdf_wm, ydf_wm, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(r2_score(y_train, model.predict(X_train)))\n",
    "print(r2_score(y_test, model.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_day['Prediction'] = model.predict(xdf_wm)\n",
    "px.line(df_day, x = 'Datum', y = ['Bookings', 'Prediction'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Das neue Modell mit einer erhöhten Anzahl an Features erreicht auf den Trainingsdaten einen Wert von 0.8945 und auf den Testdaten von 0.8893, die Features erhöhen also das Modell. Den besten R2_Score erziehlt das Modell wenn Interaktionen zwischen den Features erlaubt werden. Dies klappt unter anderem gut weil wir einen Spezialfall der Interaktion zwischen qualitativen Features welche Dummy-Kodiert sind und quantitativen Features haben. Das Modell erreicht einen R2_Score von 0.9518 auf den Trainingsdaten und 0.941 auf den Testdaten. Werden in einem Liniendiagramm die eigentlichen Werte mit den Predictions des Modells verglichen, kann erkannt werden, dass das Modell zwar nicht die gesamte Varianz der Daten erklären kann, allerdings ist die Tendenz des Modells nahe an den eigentlichen Werten und das sogar bei Ausreiserwerten. Gerade stärkere Schwankunden kann das Modell so erkennen. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "\n",
    "col = ['atemp', 'humidity','windspeed', 'year',\n",
    "       'workingday', 'weather_clear', 'weather_cloudy', 'weather_heavy', 'weather_snow', 'month']\n",
    "xdf_wm = df_reg[col]\n",
    "xdf_wm = pd.get_dummies(xdf_wm, columns=['month', 'year'])\n",
    "ydf_wm = df_reg[['Bookings']]\n",
    "\n",
    "# Interaction Term\n",
    "poly = PolynomialFeatures(interaction_only=True)\n",
    "xdf_wm = poly.fit_transform(xdf_wm)\n",
    "\n",
    "# Linear Regression\n",
    "X_train, X_test, y_train, y_test = train_test_split(xdf_wm, ydf_wm, test_size=0.33, random_state=42)\n",
    "\n",
    "model = Ridge()\n",
    "model.fit(X_train, y_train)\n",
    "print(r2_score(y_train, model.predict(X_train)))\n",
    "print(r2_score(y_test, model.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_day['Prediction'] = model.predict(xdf_wm)\n",
    "px.line(df_day, x = 'Datum', y = ['Bookings', 'Prediction'], title = 'Vergleich zwischen Modell und eigentlichen Werten')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Regression auf Stunde\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Da nun ein Modell besteht welches die täglichen Werte prognostiziert steht natürlich die Vermutung im Raum ob es auch ein Modell gibt welches die stündlichen Werte vorhersagt. Gerade da eine Abhängigkeit besteht zwischen der Tageszeit und den Bookings zu dieser Tageszeit, welche durch Bedingungen wie, ob es sich um einen Arbeitstag handelt, verstärkt werden kann.(Verweis auf vorherige Analysen.) Da die Werte zu den jeweiligen Tageszeiten stündlichen Schwankungen unterliegen werden diese durch die Bildung von Dummy-Variablen beachtet. Das Modell erreicht hier einen Score von 0.9 auf den Trainingsdaten und 0.8878 auf den Testdaten. Bei genauerer Betrachtung des Modells fällt allerdings auf, dass oftmals in den frühen Morgestunden in denen die niedrigste tägliche Nutzung vorliegt, das Modell negative Werte prognostiziert. Damit diser Effekt verhindert werden kann, werden alle Prognosen welche kleiner als 0 sind als 0 angenommen, also das zu dieser Stunde niemand fährt. Nichtsdestotrotz, wird in den Daten nach einem spezifischen Kalenderwoche gefiltert(Beispiel KW:32, 2011), sieht man, dass das Modell die stündlichen Schwankungen erkennt. Beispielsweise erkennt das Modell die Entwicklung an einem Arbeitstag und das morgentliche und abendliche Hoch in der Nutzung. Auch das Wochenende und die veränderte stündliche Nutzung am Wochenende kann hier erkannt werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummy = pd.get_dummies(df[['datetime', 'weather_name']], columns=['weather_name'])\n",
    "df_dummy = df_dummy.groupby(['datetime']).mean()\n",
    "df_dummy.index = pd.to_datetime(df_dummy.index)\n",
    "df_dummy = df_dummy.resample('h').mean()\n",
    "df_dummy = df_dummy.reset_index()\n",
    "df_dummy.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hourlyy = pd.merge(df_hourly, df_dummy, on = 'datetime')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "\n",
    "col = ['atemp', 'humidity','windspeed', 'year','workingday', \n",
    "       'month', 'hour', 'weather_name_clear, few clouds', 'weather_name_cloudy, mist', 'weather_name_heavy rain or thunderstorm or snow or ice pallets', 'weather_name_light snow or rain or thunderstorm']\n",
    "xdf_wm = df_hourlyy[col]\n",
    "ydf_wm = df_hourlyy[['Bookings']]\n",
    "xdf_wm = pd.get_dummies(xdf_wm, columns=['month', 'hour'])\n",
    "\n",
    "# Interaction Term\n",
    "poly = PolynomialFeatures(interaction_only=True)\n",
    "xdf_wm = poly.fit_transform(xdf_wm)\n",
    "\n",
    "# Linear Regression\n",
    "X_train, X_test, y_train, y_test = train_test_split(xdf_wm, ydf_wm, test_size=0.35, random_state=42)\n",
    "\n",
    "model = Ridge()\n",
    "model.fit(X_train, y_train)\n",
    "print(r2_score(y_train, model.predict(X_train)))\n",
    "print(r2_score(y_test, model.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_hourlyy['Prediction'] = model.predict(xdf_wm).round()\n",
    "df_hourlyy.head()\n",
    "def get_null(val):\n",
    "    if val < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return val\n",
    "df_hourlyy['Prediction'] = df_hourlyy['Prediction'].apply(get_null)\n",
    "print(r2_score(df_hourlyy['Bookings'], df_hourlyy['Prediction']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask = (df_hourlyy['year'] == 2011) & (df_hourlyy['month'] == 8)\n",
    "px.line(df_hourlyy[mask], x = 'datetime', y = ['Bookings', 'Prediciton'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask = (df_hourlyy['year'] == 2011) & (df_hourlyy['month'] == 8) & (df_hourlyy['week_of_year'] == 32)\n",
    "fig = px.line(df_hourlyy[mask].reset_index(), x = 'datetime', y = 'Bookings',\n",
    "        title = 'Vergleich zwischen den stündlichen Bookings und der Prognose in der Kalenderwoche 32 im Jahr 2011')\n",
    "fig.add_scatter(x = df_hourlyy[mask].reset_index()['datetime'], y = df_hourlyy[mask]['Prediction'], line = {'dash' : 'dot'}, name='Trend')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Resampling auf Tagesebene\n",
    "\n",
    "df_hourlyy = df_hourlyy.set_index('datetime')\n",
    "df_day_pred = df_hourlyy.resample('d').sum()\n",
    "df_day_pred.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "px.line(df_day_pred.reset_index(), x = 'datetime', y = ['Bookings', 'Prediciton'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r2_score(df_day_pred['Bookings'], df_day_pred['Prediction'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['atemp', 'humidity','windspeed', 'year', 'month', 'workingday']\n",
    "xdf_wm = df_day[col]\n",
    "ydf_wm = df_day[['Bookings']]\n",
    "# Interaction Term\n",
    "poly = PolynomialFeatures()\n",
    "xdf_wm = poly.fit_transform(xdf_wm)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(xdf_wm, ydf_wm, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(X_train, y_train)\n",
    "print(r2_score(y_train, model.predict(X_train)))\n",
    "print(r2_score(y_test, model.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_dummy = pd.get_dummies(df[['datetime', 'weather']])\n",
    "df_dummy['Datum'] = df_dummy.datetime.dt.date\n",
    "df_dummy = df_dummy.groupby(['Datum']).mean()\n",
    "df_dummy = df_dummy.reset_index()\n",
    "df_dummy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
